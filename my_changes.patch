diff --git a/docs/source/en/_toctree.yml b/docs/source/en/_toctree.yml
index 33c4a7df5..e9295eed3 100644
--- a/docs/source/en/_toctree.yml
+++ b/docs/source/en/_toctree.yml
@@ -1,16 +1,14 @@
-- title: Get started
-  sections:
+- sections:
   - local: index
     title: Transformers
   - local: installation
     title: Installation
   - local: quicktour
     title: Quickstart
-- title: Base classes
-  isExpanded: False
+  title: Get started
+- isExpanded: false
   sections:
-  - title: Models
-    sections:
+  - sections:
     - local: models
       title: Loading models
     - local: custom_models
@@ -31,8 +29,8 @@
       title: The Transformer model family
     - local: attention
       title: Attention mechanisms
-  - title: Preprocessors
-    sections:
+    title: Models
+  - sections:
     - local: fast_tokenizers
       title: Tokenizers
     - local: image_processors
@@ -47,11 +45,11 @@
       title: Summary of the tokenizers
     - local: pad_truncation
       title: Padding and truncation
-- title: Inference
-  isExpanded: False
+    title: Preprocessors
+  title: Base classes
+- isExpanded: false
   sections:
-  - title: Pipeline API
-    sections:
+  - sections:
     - local: pipeline_tutorial
       title: Pipeline
     - local: pipeline_gradio
@@ -60,8 +58,8 @@
       title: Web server inference
     - local: add_new_pipeline
       title: Adding a new pipeline
-  - title: LLMs
-    sections:
+    title: Pipeline API
+  - sections:
     - local: llm_tutorial
       title: Text generation
     - local: generation_strategies
@@ -82,8 +80,8 @@
       title: Getting the most out of LLMs
     - local: perplexity
       title: Perplexity of fixed-length models
-  - title: Chat with models
-    sections:
+    title: LLMs
+  - sections:
     - local: conversations
       title: Chat basics
     - local: chat_templating
@@ -94,8 +92,8 @@
       title: Template writing
     - local: chat_extras
       title: Tools and RAG
-  - title: Optimization
-    sections:
+    title: Chat with models
+  - sections:
     - local: perf_torch_compile
       title: torch.compile
     - local: perf_infer_gpu_one
@@ -106,15 +104,15 @@
       title: CPU
     - local: tf_xla
       title: XLA
+    title: Optimization
   - local: agents
     title: Agents
   - local: tools
     title: Tools
-- title: Training
-  isExpanded: False
+  title: Inference
+- isExpanded: false
   sections:
-  - title: Trainer API
-    sections:
+  - sections:
     - local: trainer
       title: Trainer
     - local: training
@@ -123,8 +121,8 @@
       title: Optimizers
     - local: hpo_train
       title: Hyperparameter search
-  - title: Distributed training
-    sections:
+    title: Trainer API
+  - sections:
     - local: gpu_selection
       title: GPU selection
     - local: accelerate
@@ -139,8 +137,8 @@
       title: Distributed CPUs
     - local: perf_train_gpu_many
       title: Parallelism methods
-  - title: Hardware
-    sections:
+    title: Distributed training
+  - sections:
     - local: perf_train_gpu_one
       title: GPU
     - local: perf_train_cpu
@@ -151,12 +149,13 @@
       title: Apple Silicon
     - local: perf_hardware
       title: Build your own machine
+    title: Hardware
   - local: peft
     title: PEFT
   - local: model_memory_anatomy
     title: Model training anatomy
-- title: Quantization
-  isExpanded: False
+  title: Training
+- isExpanded: false
   sections:
   - local: quantization/overview
     title: Overview
@@ -196,8 +195,8 @@
     title: VPTQ
   - local: quantization/contribute
     title: Contribute
-- title: Export to production
-  isExpanded: False
+  title: Quantization
+- isExpanded: false
   sections:
   - local: serialization
     title: ONNX
@@ -207,13 +206,11 @@
     title: ExecuTorch
   - local: torchscript
     title: TorchScript
-- title: Resources
-  isExpanded: False
+  title: Export to production
+- isExpanded: false
   sections:
-  - title: Task recipes
-    sections:
-    - title: Natural language processing
-      sections:
+  - sections:
+    - sections:
       - local: tasks/sequence_classification
         title: Text classification
       - local: tasks/token_classification
@@ -230,14 +227,14 @@
         title: Summarization
       - local: tasks/multiple_choice
         title: Multiple choice
-    - title: Audio
-      sections:
+      title: Natural language processing
+    - sections:
       - local: tasks/audio_classification
         title: Audio classification
       - local: tasks/asr
         title: Automatic speech recognition
-    - title: Computer vision
-      sections:
+      title: Audio
+    - sections:
       - local: tasks/image_classification
         title: Image classification
       - local: tasks/semantic_segmentation
@@ -262,8 +259,8 @@
         title: Keypoint detection
       - local: tasks/knowledge_distillation_for_image_classification
         title: Knowledge Distillation for Computer Vision
-    - title: Multimodal
-      sections:
+      title: Computer vision
+    - sections:
       - local: tasks/image_captioning
         title: Image captioning
       - local: tasks/document_question_answering
@@ -278,6 +275,8 @@
         title: Image-text-to-text
       - local: tasks/video_text_to_text
         title: Video-text-to-text
+      title: Multimodal
+    title: Task recipes
   - local: run_scripts
     title: Training scripts
   - local: glossary
@@ -290,8 +289,8 @@
     title: Community resources
   - local: troubleshooting
     title: Troubleshoot
-- title: Contribute
-  isExpanded: False
+  title: Resources
+- isExpanded: false
   sections:
   - local: contributing
     title: Contribute to Transformers
@@ -299,11 +298,10 @@
     title: Transformers model tests
   - local: pr_checks
     title: Pull request checks
-- title: API
-  isExpanded: False
+  title: Contribute
+- isExpanded: false
   sections:
-  - title: Main classes
-    sections:
+  - sections:
     - local: main_classes/agent
       title: Agents and Tools
     - local: model_doc/auto
@@ -350,10 +348,9 @@
       title: Feature Extractor
     - local: main_classes/image_processor
       title: Image Processor
-  - title: Models
-    sections:
-    - title: Text models
-      sections:
+    title: Main classes
+  - sections:
+    - sections:
       - local: model_doc/albert
         title: ALBERT
       - local: model_doc/bamba
@@ -662,8 +659,8 @@
         title: Zamba
       - local: model_doc/zamba2
         title: Zamba2
-    - title: Vision models
-      sections:
+      title: Text models
+    - sections:
       - local: model_doc/beit
         title: BEiT
       - local: model_doc/bit
@@ -790,8 +787,8 @@
         title: YOLOS
       - local: model_doc/zoedepth
         title: ZoeDepth
-    - title: Audio models
-      sections:
+      title: Vision models
+    - sections:
       - local: model_doc/audio-spectrogram-transformer
         title: Audio Spectrogram Transformer
       - local: model_doc/bark
@@ -860,16 +857,16 @@
         title: XLS-R
       - local: model_doc/xlsr_wav2vec2
         title: XLSR-Wav2Vec2
-    - title: Video models
-      sections:
+      title: Audio models
+    - sections:
       - local: model_doc/timesformer
         title: TimeSformer
       - local: model_doc/videomae
         title: VideoMAE
       - local: model_doc/vivit
         title: ViViT
-    - title: Multimodal models
-      sections:
+      title: Video models
+    - sections:
       - local: model_doc/align
         title: ALIGN
       - local: model_doc/altclip
@@ -974,6 +971,8 @@
         title: Pixtral
       - local: model_doc/qwen2_5_vl
         title: Qwen2.5-VL
+      - local: model_doc/long_vita
+        title: Long_Vita
       - local: model_doc/qwen2_audio
         title: Qwen2Audio
       - local: model_doc/qwen2_vl
@@ -1012,14 +1011,14 @@
         title: VisualBERT
       - local: model_doc/xclip
         title: X-CLIP
-    - title: Reinforcement learning models
-      sections:
+      title: Multimodal models
+    - sections:
       - local: model_doc/decision_transformer
         title: Decision Transformer
       - local: model_doc/trajectory_transformer
         title: Trajectory Transformer
-    - title: Time series models
-      sections:
+      title: Reinforcement learning models
+    - sections:
       - local: model_doc/autoformer
         title: Autoformer
       - local: model_doc/informer
@@ -1030,12 +1029,13 @@
         title: PatchTST
       - local: model_doc/time_series_transformer
         title: Time Series Transformer
-    - title: Graph models
-      sections:
+      title: Time series models
+    - sections:
       - local: model_doc/graphormer
         title: Graphormer
-  - title: Internal helpers
-    sections:
+      title: Graph models
+    title: Models
+  - sections:
     - local: internal/modeling_utils
       title: Custom Layers and Utilities
     - local: internal/pipelines_utils
@@ -1054,4 +1054,5 @@
       title: General Utilities
     - local: internal/time_series_utils
       title: Utilities for Time Series
-      
\ No newline at end of file
+    title: Internal helpers
+  title: API
diff --git a/src/transformers/__init__.py b/src/transformers/__init__.py
index 12ac2c60d..30e8a563b 100755
--- a/src/transformers/__init__.py
+++ b/src/transformers/__init__.py
@@ -723,6 +723,11 @@ _import_structure = {
         "Qwen2_5_VLConfig",
         "Qwen2_5_VLProcessor",
     ],
+    "models.long_vita": [
+        "Long_vitaConfig",
+        "Long_vitaProcessor",
+        "Long_vitaVisionConfig",
+    ],
     "models.qwen2_audio": [
         "Qwen2AudioConfig",
         "Qwen2AudioEncoderConfig",
@@ -3353,6 +3358,13 @@ else:
             "Qwen2_5_VLPreTrainedModel",
         ]
     )
+    _import_structure["models.long_vita"].extend(
+        [
+            "Long_vitaForConditionalGeneration",
+            "Long_vitaPreTrainedModel",
+            "Long_vitaVisionModel",
+        ]
+    )
     _import_structure["models.qwen2_audio"].extend(
         [
             "Qwen2AudioEncoder",
@@ -5906,6 +5918,11 @@ if TYPE_CHECKING:
         Qwen2_5_VLConfig,
         Qwen2_5_VLProcessor,
     )
+    from .models.long_vita import (
+        Long_vitaConfig,
+        Long_vitaProcessor,
+        Long_vitaVisionConfig,
+    )
     from .models.qwen2_audio import (
         Qwen2AudioConfig,
         Qwen2AudioEncoderConfig,
@@ -8154,6 +8171,11 @@ if TYPE_CHECKING:
             Qwen2_5_VLModel,
             Qwen2_5_VLPreTrainedModel,
         )
+        from .models.long_vita import (
+            Long_vitaForConditionalGeneration,
+            Long_vitaPreTrainedModel,
+            Long_vitaVisionModel,
+        )
         from .models.qwen2_audio import (
             Qwen2AudioEncoder,
             Qwen2AudioForConditionalGeneration,
diff --git a/src/transformers/generation/utils.py b/src/transformers/generation/utils.py
index 5c7fffb11..9406dcd0a 100644
--- a/src/transformers/generation/utils.py
+++ b/src/transformers/generation/utils.py
@@ -1416,11 +1416,11 @@ class GenerationMixin:
             if value is not None and key not in model_args:
                 unused_model_args.append(key)
 
-        if unused_model_args:
-            raise ValueError(
-                f"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the"
-                " generate arguments will also show up in this list)"
-            )
+        # if unused_model_args:
+        #     raise ValueError(
+        #         f"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the"
+        #         " generate arguments will also show up in this list)"
+        #     )
 
     def _validate_generated_length(self, generation_config, input_ids_length, has_default_max_length):
         """Performs validation related to the resulting generated length"""
diff --git a/src/transformers/image_processing_base.py b/src/transformers/image_processing_base.py
index a6ce7af3f..42d9da521 100644
--- a/src/transformers/image_processing_base.py
+++ b/src/transformers/image_processing_base.py
@@ -338,6 +338,7 @@ class ImageProcessingMixin(PushToHubMixin):
             image_processor_file = image_processor_filename
             try:
                 # Load from local folder or from cache or download from model Hub and cache
+                local_files_only = True
                 resolved_image_processor_file = cached_file(
                     pretrained_model_name_or_path,
                     image_processor_file,
diff --git a/src/transformers/image_utils.py b/src/transformers/image_utils.py
index fec1e9dbc..e127b6267 100644
--- a/src/transformers/image_utils.py
+++ b/src/transformers/image_utils.py
@@ -21,6 +21,7 @@ from io import BytesIO
 from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Union
 
 import numpy as np
+import torch
 import requests
 from packaging import version
 
@@ -160,6 +161,17 @@ def is_valid_list_of_images(images: List):
     return images and all(is_valid_image(image) for image in images)
 
 
+def concatenate_list(input_list):
+    if isinstance(input_list[0], list):
+        input_list = [item for sublist in input_list for item in sublist]
+    if isinstance(input_list[0], np.ndarray):
+        return np.concatenate(input_list, axis=0)
+    elif isinstance(input_list[0], torch.Tensor):
+        return torch.cat(input_list, dim=0)
+    else:
+        raise ValueError(f"Unsupported type {type(input_list[0])}")
+        
+
 def valid_images(imgs):
     # If we have an list of images, make sure every image is valid
     if isinstance(imgs, (list, tuple)):
diff --git a/src/transformers/models/__init__.py b/src/transformers/models/__init__.py
index 3884daabd..8d9416774 100644
--- a/src/transformers/models/__init__.py
+++ b/src/transformers/models/__init__.py
@@ -222,6 +222,7 @@ from . import (
     pvt_v2,
     qwen2,
     qwen2_5_vl,
+    long_vita,
     qwen2_audio,
     qwen2_moe,
     qwen2_vl,
diff --git a/src/transformers/models/auto/configuration_auto.py b/src/transformers/models/auto/configuration_auto.py
index fa4de1955..5d408a819 100644
--- a/src/transformers/models/auto/configuration_auto.py
+++ b/src/transformers/models/auto/configuration_auto.py
@@ -244,6 +244,8 @@ CONFIG_MAPPING_NAMES = OrderedDict(
         ("qdqbert", "QDQBertConfig"),
         ("qwen2", "Qwen2Config"),
         ("qwen2_5_vl", "Qwen2_5_VLConfig"),
+        ("long_vita", "Long_vitaConfig"),
+        ("long_vita_vision", "Long_vitaVisionConfig"),
         ("qwen2_audio", "Qwen2AudioConfig"),
         ("qwen2_audio_encoder", "Qwen2AudioEncoderConfig"),
         ("qwen2_moe", "Qwen2MoeConfig"),
@@ -592,6 +594,8 @@ MODEL_NAMES_MAPPING = OrderedDict(
         ("qdqbert", "QDQBert"),
         ("qwen2", "Qwen2"),
         ("qwen2_5_vl", "Qwen2_5_VL"),
+        ("long_vita", "Long_vita"),
+        ("long_vita_vision", "Long_vitaVision"),
         ("qwen2_audio", "Qwen2Audio"),
         ("qwen2_audio_encoder", "Qwen2AudioEncoder"),
         ("qwen2_moe", "Qwen2MoE"),
@@ -754,6 +758,7 @@ SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict(
         ("chinese_clip_vision_model", "chinese_clip"),
         ("rt_detr_resnet", "rt_detr"),
         ("granitevision", "llava_next"),
+        ("long_vita_vision", "Long_vitaVision"),
     ]
 )
 
diff --git a/src/transformers/models/auto/modeling_auto.py b/src/transformers/models/auto/modeling_auto.py
index d9fd502c1..3cf9d594d 100644
--- a/src/transformers/models/auto/modeling_auto.py
+++ b/src/transformers/models/auto/modeling_auto.py
@@ -160,6 +160,7 @@ MODEL_MAPPING_NAMES = OrderedDict(
         ("llama", "LlamaModel"),
         ("longformer", "LongformerModel"),
         ("longt5", "LongT5Model"),
+        ("long_vita_vision", "Long_vitaVisionModel"),
         ("luke", "LukeModel"),
         ("lxmert", "LxmertModel"),
         ("m2m_100", "M2M100Model"),
@@ -226,6 +227,7 @@ MODEL_MAPPING_NAMES = OrderedDict(
         ("qdqbert", "QDQBertModel"),
         ("qwen2", "Qwen2Model"),
         ("qwen2_5_vl", "Qwen2_5_VLModel"),
+        ("long_vita", "Long_vitaModel"),
         ("qwen2_audio_encoder", "Qwen2AudioEncoder"),
         ("qwen2_moe", "Qwen2MoeModel"),
         ("qwen2_vl", "Qwen2VLModel"),
@@ -802,6 +804,7 @@ MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = OrderedDict(
         ("paligemma", "PaliGemmaForConditionalGeneration"),
         ("pix2struct", "Pix2StructForConditionalGeneration"),
         ("qwen2_5_vl", "Qwen2_5_VLForConditionalGeneration"),
+        ("long_vita", "Long_vitaForConditionalGeneration"),
         ("qwen2_vl", "Qwen2VLForConditionalGeneration"),
         ("video_llava", "VideoLlavaForConditionalGeneration"),
         ("vipllava", "VipLlavaForConditionalGeneration"),
@@ -839,6 +842,7 @@ MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES = OrderedDict(
         ("pix2struct", "Pix2StructForConditionalGeneration"),
         ("pixtral", "LlavaForConditionalGeneration"),
         ("qwen2_5_vl", "Qwen2_5_VLForConditionalGeneration"),
+        ("long_vita", "Long_vitaForConditionalGeneration"),
         ("qwen2_vl", "Qwen2VLForConditionalGeneration"),
         ("smolvlm", "SmolVLMForConditionalGeneration"),
         ("udop", "UdopForConditionalGeneration"),
diff --git a/src/transformers/models/auto/processing_auto.py b/src/transformers/models/auto/processing_auto.py
index 2d6da5ac1..5aa631caf 100644
--- a/src/transformers/models/auto/processing_auto.py
+++ b/src/transformers/models/auto/processing_auto.py
@@ -93,6 +93,7 @@ PROCESSOR_MAPPING_NAMES = OrderedDict(
         ("pixtral", "PixtralProcessor"),
         ("pop2piano", "Pop2PianoProcessor"),
         ("qwen2_5_vl", "Qwen2_5_VLProcessor"),
+        ("long_vita", "Long_vitaProcessor"),
         ("qwen2_audio", "Qwen2AudioProcessor"),
         ("qwen2_vl", "Qwen2VLProcessor"),
         ("sam", "SamProcessor"),
diff --git a/src/transformers/models/auto/tokenization_auto.py b/src/transformers/models/auto/tokenization_auto.py
index 57bcd3129..06347b43d 100644
--- a/src/transformers/models/auto/tokenization_auto.py
+++ b/src/transformers/models/auto/tokenization_auto.py
@@ -283,6 +283,7 @@ else:
                     "T5TokenizerFast" if is_tokenizers_available() else None,
                 ),
             ),
+            ("long_vita", ("Qwen2Tokenizer", "w3" if is_tokenizers_available() else None)),
             ("luke", ("LukeTokenizer", None)),
             ("lxmert", ("LxmertTokenizer", "LxmertTokenizerFast" if is_tokenizers_available() else None)),
             ("m2m_100", ("M2M100Tokenizer" if is_sentencepiece_available() else None, None)),
diff --git a/src/transformers/pipelines/base.py b/src/transformers/pipelines/base.py
index d3ee4e871..624a4b6c9 100644
--- a/src/transformers/pipelines/base.py
+++ b/src/transformers/pipelines/base.py
@@ -1487,4 +1487,4 @@ class PipelineRegistry:
         pipeline_class._registered_impl = {task: task_impl}
 
     def to_dict(self):
-        return self.supported_tasks
+        return self.supported_tasks
\ No newline at end of file
diff --git a/src/transformers/pipelines/image_text_to_text.py b/src/transformers/pipelines/image_text_to_text.py
index 6b743997f..a92e2f3c6 100644
--- a/src/transformers/pipelines/image_text_to_text.py
+++ b/src/transformers/pipelines/image_text_to_text.py
@@ -59,7 +59,6 @@ class Chat:
             if not ("role" in message and "content" in message):
                 raise ValueError("When passing chat dicts as input, each dict must have a 'role' and 'content' key.")
         images = retrieve_images_in_messages(messages, images)
-
         self.messages = messages
         self.images = images
 
@@ -321,7 +320,7 @@ class ImageTextToTextPipeline(Pipeline):
                 "The input data was not formatted as a chat with dicts containing 'role' and 'content' keys, even though this model supports chat. "
                 "Consider using the chat format for better results. For more information, see https://huggingface.co/docs/transformers/en/chat_templating"
             )
-
+        
         # support text only generation
         if images is None:
             return super().__call__(text, **kwargs)
diff --git a/utils/modular_model_converter.py b/utils/modular_model_converter.py
index 728c5628b..22b13dc48 100644
--- a/utils/modular_model_converter.py
+++ b/utils/modular_model_converter.py
@@ -1252,11 +1252,11 @@ class ModularFileMapper(ModuleMapper):
                         self.model_specific_modules[import_module] = tree
                     imported_object = self.python_module.code_for_node(imported_.name)
                     self.model_specific_imported_objects[imported_object] = import_module
-        if m.matches(node.module, m.Name()):
-            if "transformers" == import_module:
-                raise ValueError(
-                    f"You are importing from {import_module} directly using global imports. Import from the correct local path"
-                )
+        # if m.matches(node.module, m.Name()):
+        #     if "transformers" == import_module:
+        #         raise ValueError(
+        #             f"You are importing from {import_module} directly using global imports. Import from the correct local path"
+        #         )
 
     def visit_SimpleStatementLine(self, node):
         """If we visit an import statement not previously visited, record it. If we visit a module-scope assignment,
